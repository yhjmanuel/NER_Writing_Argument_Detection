{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsIF3MSBVdtC",
    "outputId": "44f727e7-85d5-4d8a-b62b-31337d65d0a4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import os\n",
    "from typing import NamedTuple, Sequence, Any, List\n",
    "import string\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import LongformerForTokenClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration, data loading & spliting, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OX5-Mt-OV-ZV",
    "outputId": "c786403e-fd1b-4660-cf92-3db49c3db594"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Set the training configurations.\n",
    "    \"\"\"\n",
    "    n_classes = 15\n",
    "    n_epochs = 3\n",
    "    lr = 1e-5\n",
    "    model = LongformerForTokenClassification.from_pretrained('allenai/longformer-base-4096', num_labels=15)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                              factor=0.9,\n",
    "                                                              mode=\"min\",\n",
    "                                                              patience=10,\n",
    "                                                              cooldown=10,\n",
    "                                                              min_lr=5e-6,\n",
    "                                                              verbose=True)\n",
    "    # why setting batch_size = 1? Because even setting batch = 2 will result in a \n",
    "    # CUDA out of memory error\n",
    "    train_batch_size = 32\n",
    "    dev_batch_size = 32\n",
    "    test_batch_size = 32\n",
    "    train_split = 0.8\n",
    "    #tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('drive/MyDrive/NER_project/longformer_train_set.pickle', 'rb') as train:\n",
    "    train_data = pickle.load(train)\n",
    "with open('drive/MyDrive/NER_project/longformer_dev_set.pickle', 'rb') as dev:\n",
    "    dev_data = pickle.load(dev)\n",
    "with open('drive/MyDrive/NER_project/longformer_test_set.pickle', 'rb') as test:\n",
    "    test_data = pickle.load(test)\n",
    "train_set = DataLoader(train_data, batch_size=Config.train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_set = DataLoader(dev_data, batch_size=Config.dev_batch_size, shuffle=True, pin_memory=True)\n",
    "test_set = DataLoader(test_data, batch_size=Config.test_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFkND2F_WhNh"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(Config, train_set, dev_set, test_set, save_model_patj='longformer_model.pt')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfH7t7ZO8Ep6"
   },
   "source": [
    "## Evaluate our model on the test set (token-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90Uz3xl3qT6L",
    "outputId": "0919ff11-5141-4326-8799-8cd0a56fa85d"
   },
   "outputs": [],
   "source": [
    "trainer.run_on_dev_or_test(dataset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYc-6m7n-O9K"
   },
   "source": [
    "## Get mention-level micro F1 on the test set\n",
    "\n",
    "\n",
    "In this project, the mentions are defined on sentence-level.\n",
    "For instance, there is an article, which consists of only 3 sentences:\n",
    "\n",
    "\"This is the beginning of the article, this is the evidence of the article, and this is the end of the article.\"\n",
    "\n",
    "We define a sentence as a part of the text delimited by one of the following punctuations: a period, a question mark, an exclamation point, a colon, a comma, or a semicolon. Therefore, in the example above, there are three sentences, and their corresponding labels are LEAD, EVIDENCE, and CONCLUDING STATEMENT, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-WC93n20Ii2"
   },
   "outputs": [],
   "source": [
    "# load the held-out untokenized test set data, which is useful for computing mention-level F1 \n",
    "with open('original_test_set.pickle', 'rb') as t:\n",
    "    final_test_data = pickle.load(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we only have a label for every subtoken. How do we get the label for each sentence?\n",
    "1. Convert one label for one subtoken to one label for one token (delimited by a whitespace) in the article\n",
    "2. If a sentence's position is at the first or last 20% of an article, assign it the most frequent label of its tokens.\n",
    "3. If a sentence's position is at the middle 60% of an article: if the second most frequent label of its tokens account for more than 20% of the number of labels in this sentence, use the second most frequent label. Otherwise, use the most frequent label of its tokens. In the training data, \"EVIDENCE\" is so predominant, this strategy helps to prevent the model from producing too many \"EVIDENCEs\".\n",
    "\n",
    "Here is an example. Assume that below is a sentence at the middle 60% of an article:\n",
    "This [B-EVIDENCE] is [I-EVIDENCE] a [I-CLAIM] claim [I-CLAIM] of [I-EVIDENCE] an [I-EVIDENCE] article [I-EVIDENCE]\n",
    "\n",
    "It will be classified as a CLAIM, because CLAIM is the second most frequent label of the sentence's tokens, and it accounts for more than 20% of the number of labels in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xCqvnXt-zka"
   },
   "outputs": [],
   "source": [
    "def inference(test_txt, config, model, max_length):\n",
    "    # given a tokenized article, get each sentence's label\n",
    "    # define our label to number dictionary\n",
    "    label_to_num = {'O': 0,\n",
    "                    'B-LEAD': 1,\n",
    "                    'I-LEAD': 2,\n",
    "                    'B-POSITION': 3,\n",
    "                    'I-POSITION': 4,\n",
    "                    'B-CLAIM': 5,\n",
    "                    'I-CLAIM': 6,\n",
    "                    'B-COUNTERCLAIM': 7,\n",
    "                    'I-COUNTERCLAIM': 8,\n",
    "                    'B-REBUTTAL': 9,\n",
    "                    'I-REBUTTAL': 10,\n",
    "                    'B-EVIDENCE': 11,\n",
    "                    'I-EVIDENCE': 12,\n",
    "                    'B-CS': 13,\n",
    "                    'I-CS': 14}\n",
    "    num_to_label = {label_to_num[key]: key for key in label_to_num}\n",
    "    article = ' '.join(test_txt)\n",
    "  \n",
    "    processed_length = 0\n",
    "    total_length = len(article)\n",
    "    # store the normalized labels and the list of tokens\n",
    "    normalized_labels = []\n",
    "    tokens_list = []\n",
    "    left_article = article\n",
    "    # an article can be very long, \n",
    "    # therefore it may be split into several parts and processed several times\n",
    "    # we combine the predicted result of each time\n",
    "    # if we use longformer tokenizer, we don't need to worry about this problem, because max length = 4096\n",
    "    # and we can process an article in one time\n",
    "    # but we need to consider this when using roberta tokenizer, which has max length = 512\n",
    "    while processed_length < total_length:\n",
    "   \n",
    "        last_token_pos = 0\n",
    "        encoding = config.tokenizer(left_article, padding='max_length', truncation=True, return_offsets_mapping=True)\n",
    "        ids = torch.tensor(encoding['input_ids']).reshape(1, -1).to(config.device)\n",
    "        mask = torch.tensor(encoding['attention_mask']).reshape(1, -1).to(config.device)\n",
    "        logits = model(input_ids=ids, attention_mask = mask).logits\n",
    "        logits = torch.argmax(logits.view(-1, model.num_labels), axis=1).cpu().numpy()\n",
    "        #get label for every sub-token\n",
    "        predictions = [num_to_label[i] for i in logits]\n",
    "\n",
    "        for i in range(max_length-1, 0, -1):\n",
    "            if encoding['offset_mapping'][i][1] != 0:\n",
    "        # find how many tokens have been processed\n",
    "        # using the offset_mapping attribute in the truncated result\n",
    "                last_token_pos = encoding['offset_mapping'][i][1]\n",
    "                processed_length += last_token_pos + 1\n",
    "                break\n",
    "        temp_article = left_article[:last_token_pos]\n",
    "        left_article = article[processed_length:]\n",
    "\n",
    "      # find the index of every char and its corresponding label\n",
    "        char_label_dic = {}\n",
    "        for i in range(len(encoding['offset_mapping'])):\n",
    "            for j in range(encoding['offset_mapping'][i][0], encoding['offset_mapping'][i][1] + 1):\n",
    "                char_label_dic[j] = predictions[i]\n",
    "      # normalize the subtokens' labels using the char label dictionary \n",
    "      # one label for every subtoken -> one label for every token split by whitespace in the article\n",
    "        normalized_label = []\n",
    "        curr_len = 0\n",
    "        token_list = temp_article.split(' ')\n",
    "        for token in token_list:\n",
    "            curr = char_label_dic[curr_len + len(token) // 2]\n",
    "            if curr == 'O':\n",
    "                normalized_label.append(curr)\n",
    "            else:\n",
    "                normalized_label.append(curr[2:])\n",
    "            curr_len = curr_len + len(token) + 1\n",
    "        normalized_labels.extend(normalized_label)\n",
    "        tokens_list.extend(token_list)\n",
    "  \n",
    "    # one label for one sentence\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    # temp variable, for spliting an article into multiple sentences. \n",
    "    # We normalize the labels by assigning a label to each sentence\n",
    "    sentence = []\n",
    "    label = []\n",
    "    # delimiters of a sentence\n",
    "    puncts_list = ['.', ',',';', '!','?',':']\n",
    "    # our rule-based label normalization\n",
    "    for i in range(len(tokens_list)):\n",
    "        sentence.append(tokens_list[i])\n",
    "        label.append(normalized_labels[i])\n",
    "        for punct in puncts_list:\n",
    "            if punct in tokens_list[i]:\n",
    "                if len(sentence) > 3:\n",
    "                    sentences.append(' '.join(sentence))\n",
    "                    counter = Counter(label)\n",
    "                if i <= len(token_list) * 0.2 or i >= len(token_list) * 0.8:\n",
    "                    labels.append(counter.most_common(1)[0][0])\n",
    "                else:\n",
    "                    if len(counter) >= 2 and counter.most_common(2)[1][1] >= len(sentence) * 0.2:\n",
    "                        labels.append(counter.most_common(2)[1][0])\n",
    "              # if len(counter) >= 2 and counter.most_common(2)[1][1] > 1:\n",
    "              #   labels.append(counter.most_common(2)[1][0])\n",
    "                    else:\n",
    "                        labels.append(counter.most_common(1)[0][0])\n",
    "                sentence = []\n",
    "                label = []\n",
    "                break\n",
    "    return labels, sentences\n",
    "\n",
    "# given an AnnotatedDoc instance, return its correct, normalized label sequences\n",
    "# This part of code is similar to the \"test\" method\n",
    "def get_correct_labels(final_test_data):\n",
    "    labels = encode_bio(final_test_data.tokens, final_test_data.mentions)\n",
    "    normalized_labels = []\n",
    "    for i in labels:\n",
    "        if i[0] != 'B':\n",
    "            normalized_labels.append(i)\n",
    "        else:\n",
    "            normalized_labels.append('I'+i[1:])\n",
    "  \n",
    "    #get the final predictions\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    # temp, for spliting an article into multiple sentences. We normalize the labels by assigning a label to each sentence\n",
    "    sentence = []\n",
    "    label = []\n",
    "    puncts_list = ['.', ',',';', '!','?',':']\n",
    "    for i in range(len(final_test_data.tokens)):\n",
    "    # separator of a sentence\n",
    "        sentence.append(final_test_data.tokens[i])\n",
    "        label.append(normalized_labels[i])\n",
    "    for punct in puncts_list:\n",
    "        if punct in final_test_data.tokens[i]:\n",
    "            if len(sentence) > 3:\n",
    "                sentences.append(' '.join(sentence))\n",
    "                label_to_add = label[-1].upper()\n",
    "                if label_to_add == 'O':\n",
    "                    labels.append(label_to_add)\n",
    "                elif label_to_add == \"I-CONCLUDING STATEMENT\":\n",
    "                    labels.append('CS')\n",
    "                else:\n",
    "                    labels.append(label_to_add[2:])\n",
    "                sentence = []\n",
    "                label = []\n",
    "                break\n",
    "    return labels, sentences\n",
    "\n",
    "# get the mention(sentence)-level f1\n",
    "def get_mention_f1(final_test_data, config, model, max_length=4096):\n",
    "    model.to(config.device)\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for i in range(len(final_test_data)):  \n",
    "        predicted_labels, _ = inference(final_test_data[i].tokens, config, model, max_length=max_length)\n",
    "        real_labels, _ = get_correct_labels(final_test_data[i])\n",
    "        predicted.extend(predicted_labels)\n",
    "        actual.extend(real_labels)\n",
    "    print(f1_score(actual, predicted, average='micro'))\n",
    "    print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vlyW2JeYGn_",
    "outputId": "a804210c-c8af-4a93-b2d8-bedadafb05ad"
   },
   "outputs": [],
   "source": [
    "model = LongformerForTokenClassification.from_pretrained('allenai/longformer-base-4096', num_labels=15)\n",
    "model.load_state_dict(torch.load('longformer_model.pt'))\n",
    "get_mention_f1(final_test_data, Config, model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "文章切割训练代码-longformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
